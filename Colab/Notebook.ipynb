{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5yLNdDV4x5C",
    "outputId": "28e77391-cdaf-437f-b5b5-2bd202124dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.3.0\n",
      "Hub version: 0.10.0\n",
      "WARNING:tensorflow:From <ipython-input-1-0831fa394ed3>:12: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5W5cQz546N7",
    "outputId": "72e89044-62ec-4b1e-c9ad-0ad08187fe61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4 with input size (224, 224)\n"
     ]
    }
   ],
   "source": [
    "module_selection = (\"mobilenet_v2_100_224\", 224) \n",
    "handle_base, pixels = module_selection\n",
    "MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(handle_base)\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
    "\n",
    "BATCH_SIZE = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zs4Yna5Z5bZW"
   },
   "outputs": [],
   "source": [
    "data_dir = \"/content/drive/MyDrive/binary_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "YUiiA74F_yuL"
   },
   "outputs": [],
   "source": [
    "%mkdir -p /content/data/positive/train\n",
    "%mkdir -p /content/data/positive/validation\n",
    "%mkdir -p /content/data/negative/train\n",
    "%mkdir -p /content/data/negative/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "o4UL6nz1DW7n"
   },
   "outputs": [],
   "source": [
    "%cp -r '{data_dir}'/* /content/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "fM-6uOdEBN3z"
   },
   "outputs": [],
   "source": [
    "%cp /content/data/train/positives/* /content/data/positive/train/\n",
    "%cp /content/data/validation/positives/* /content/data/positive/validation/\n",
    "%cp /content/data/train/negatives/* /content/data/negative/train/\n",
    "%cp /content/data/validation/negatives/* /content/data/negative/validation/\n",
    "%rm -r /content/data/train/\n",
    "%rm -r /content/data/validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvcz5um25Z6_",
    "outputId": "4b14fa62-1cc7-4a95-dba9-ca50bfb1a34d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 403 images belonging to 2 classes.\n",
      "Found 1614 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
    "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
    "                   interpolation=\"bilinear\")\n",
    "\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **datagen_kwargs)\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\n",
    "\n",
    "do_data_augmentation = False \n",
    "if do_data_augmentation:\n",
    "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      horizontal_flip=True,\n",
    "      width_shift_range=0.2, height_shift_range=0.2,\n",
    "      shear_range=0.2, zoom_range=0.2,\n",
    "      **datagen_kwargs)\n",
    "else:\n",
    "  train_datagen = valid_datagen\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "OPxTiPJq5hZI"
   },
   "outputs": [],
   "source": [
    "do_fine_tuning = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eI_YauKn5k1i",
    "outputId": "74e7b06b-2b75-4ad7-8b7e-fdfa6926f83f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2562      \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model with\", MODULE_HANDLE)\n",
    "model = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "KVyz_OPR5mPd"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OszxZO15oQ-",
    "outputId": "bcf795fc-c325-4790-f5c6-518941468031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "17/50 [=========>....................] - ETA: 32s - loss: 0.5399 - accuracy: 0.8640"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=validation_steps).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNUno5Td51bJ"
   },
   "outputs": [],
   "source": [
    "def get_class_string_from_index(index):\n",
    "   for class_string, class_index in valid_generator.class_indices.items():\n",
    "      if class_index == index:\n",
    "         return class_string\n",
    "\n",
    "x, y = next(valid_generator)\n",
    "image = x[0, :, :, :]\n",
    "true_index = np.argmax(y[0])\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
    "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(\"True label: \" + get_class_string_from_index(true_index))\n",
    "print(\"Predicted label: \" + get_class_string_from_index(predicted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW1vKd3T58Or"
   },
   "outputs": [],
   "source": [
    "saved_model_path = \"/tmp/saved_flowers_model\"\n",
    "tf.saved_model.save(model, saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6o6U5x_6CVT"
   },
   "outputs": [],
   "source": [
    "optimize_lite_model = False  \n",
    "\n",
    "num_calibration_examples = 60  \n",
    "representative_dataset = None\n",
    "if optimize_lite_model and num_calibration_examples:\n",
    "  # Use a bounded number of training examples without labels for calibration.\n",
    "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
    "  representative_dataset = lambda: itertools.islice(\n",
    "      ([image[None, ...]] for batch, _ in train_generator for image in batch),\n",
    "      num_calibration_examples)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "if optimize_lite_model:\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  if representative_dataset:  # This is optional, see above.\n",
    "    converter.representative_dataset = representative_dataset\n",
    "lite_model_content = converter.convert()\n",
    "\n",
    "with open(\"/tmp/lite_flowers_model\", \"wb\") as f:\n",
    "  f.write(lite_model_content)\n",
    "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
    "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdVDLJg76Nih"
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
    "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
    "def lite_model(images):\n",
    "  interpreter.allocate_tensors()\n",
    "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
    "  interpreter.invoke()\n",
    "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FB4tGC8C6QyW"
   },
   "outputs": [],
   "source": [
    "num_eval_examples = 50  \n",
    "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
    "                for batch in train_generator\n",
    "                for (image, label) in zip(*batch))\n",
    "count = 0\n",
    "count_lite_tf_agree = 0\n",
    "count_lite_correct = 0\n",
    "for image, label in eval_dataset:\n",
    "  probs_lite = lite_model(image[None, ...])[0]\n",
    "  probs_tf = model(image[None, ...]).numpy()[0]\n",
    "  y_lite = np.argmax(probs_lite)\n",
    "  y_tf = np.argmax(probs_tf)\n",
    "  y_true = np.argmax(label)\n",
    "  count +=1\n",
    "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
    "  if y_lite == y_true: count_lite_correct += 1\n",
    "  if count >= num_eval_examples: break\n",
    "print(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
    "print(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wivMxHv6Tc4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AI_fire_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
